---
title: "GunnerBench"
subtitle: "A Framework for Evaluating AI in the Legal Domain"
version: "v0"
date: 2024-08-31
author: "John Benson"
authorUrl: "https://john-benson.com"
license: "CC-BY-4.0"
---

GunnerBench is a novel benchmark system designed to evaluate the performance of large language models (LLMs) in legal applications. Unlike existing benchmarks that focus on academic metrics or general capabilities, GunnerBench adopts a practical, task-oriented approach inspired by capture-the-flag competitions.  By providing a clear scoring and tiering system, GunnerBench aims to empower legal professionals to make informed decisions about AI tool selection and implementation.

I'm keeping all of the [results, discussions, and the leaderboards over on my Notion instance.](https://field-fall-b93.notion.site/GunnerBench-3e0fe64fb8714eb383b03f620f9dd6e6) 

## Introduction

### Background

The rapid advancement of artificial intelligence, particularly in the domain of large language models, has sparked significant interest in their application to legal practice. While these models demonstrate impressive capabilities in general language tasks, their specific performance in complex legal scenarios remains understudied. Existing benchmarks often fail to capture the nuanced requirements of legal work, leaving a gap in our understanding of how these models might perform in real-world legal applications.

### Motivation for GunnerBench

GunnerBench emerges from the need for a specialized evaluation framework that addresses the unique challenges of legal AI. Traditional benchmarks, while valuable for general assessment, fall short in providing insights relevant to legal practitioners. GunnerBench aims to fill this gap by offering a comprehensive, task-oriented evaluation system that mirrors the complexities of legal work.

The legal industry stands at a crossroads, with AI tools promising to revolutionize various aspects of practice. However, the adoption of these tools is hindered by uncertainty about their capabilities and limitations. GunnerBench seeks to provide clarity in this landscape, offering legal professionals a reliable means to assess and compare different AI models for specific legal tasks.

### Objectives of GunnerBench

1. Comprehensive Evaluation: GunnerBench aims to provide a holistic assessment of AI models across a wide range of legal tasks, from memo writing to document review and pleading analysis.
2. Practical Insights: By focusing on task-oriented evaluations, GunnerBench seeks to offer actionable insights that can guide the selection and implementation of AI tools in legal practice.
3. Informed Decision-Making: Through its scoring and tiering system, GunnerBench empowers legal professionals to make data-driven decisions about which AI models are best suited for their specific needs and budget constraints.
4. Innovation Driver: Through transparent evaluation and leaderboard rankings, GunnerBench aims to spur innovation in legal AI development, challenging model creators to improve performance in specific legal domains.
5. Ethical Considerations: GunnerBench incorporates ethical considerations into its design, addressing concerns about bias, privacy, and the appropriate use of AI in legal contexts.
6. Accessibility: By providing clear, comparable results, GunnerBench aims to demystify legal AI capabilities for practitioners, researchers, and decision-makers in the legal industry.

## Methodology

### Benchmark Design Philosophy

GunnerBench adopts a unique approach to AI evaluation, drawing inspiration from capture-the-flag competitions in cybersecurity. This framework emphasizes practical problem-solving and the ability to extract key information from complex legal scenarios. The benchmark is designed to challenge models not just on their general language capabilities, but on their ability to navigate the specific demands of legal tasks.

Key principles of the GunnerBench design include:

1. Task Orientation: Each evaluation category focuses on specific legal tasks, mirroring real-world applications of AI in law.
2. Precision Focus: Models are evaluated on their ability to accurately extract and analyze relevant information from legal documents.
3. Multifaceted Challenges: Assignments are designed to test various aspects of legal reasoning and analysis simultaneously.
4. Ethical Awareness: The benchmark incorporates considerations of bias, fairness, and ethical use of AI in legal contexts.
5. Practical Applicability: Results are presented in a way that directly informs decision-making in legal practice.

### Category Development

GunnerBench categories are carefully crafted to cover a broad spectrum of legal tasks. The process of developing new categories involves:

1. Identifying key areas of legal practice where AI could have significant impact
2. Defining specific tasks and subtasks within each area
3. Establishing clear evaluation criteria that reflect the nuances of the legal work involved
4. Creating a scoring system that balances quantitative metrics with qualitative assessment

Current categories include memo writing, document review, and pleading analysis, with plans to expand into additional areas of legal practice.

### Assignment Creation

Assignments within each category are designed to provide challenging, realistic scenarios that test the limits of AI capabilities in legal contexts. The assignment creation process involves:

1. Developing synthetic legal documents and scenarios that mirror the complexity of real-world cases
2. Creating comprehensive "gold standard" answers that serve as the benchmark for model performance
3. Designing prompts that elicit specific types of legal analysis or information extraction
4. Incorporating "needles in haystacks" - subtle details that require deep analysis to identify

### Scoring and Tiering System

GunnerBench employs a nuanced scoring and tiering system designed to provide clear, actionable insights for legal professionals. This system is a cornerstone of GunnerBench's ability to inform decision-making in AI tool selection.

#### Scoring System

Each assignment uses a 100-point scoring system, typically broken down as follows:

- 50 points for precision (accuracy of extracted information)
- 50 points for recall (completeness of extracted information)
- Potential bonus points for identifying cross-domain legal issues or demonstrating exceptional insight

This granular scoring allows for detailed comparison between models and helps identify specific strengths and weaknesses in different legal tasks.

#### Tiering System

Based on their performance across categories, models are assigned to tiers:

- S-Tier: Exceptional performance across most legal tasks
- A-Tier: Strong performance in many legal tasks
- B-Tier: Good performance in some legal tasks
- C-Tier: Adequate performance in basic legal tasks

This tiering system provides a quick reference for overall model capability, allowing legal professionals to quickly narrow down their options based on their specific needs and budget constraints.

#### Informing Decision-Making

The combination of detailed scoring and clear tiering enables legal professionals to:

1. Identify the most suitable models for specific legal tasks
2. Understand the trade-offs between model performance and cost
3. Make informed decisions about which AI tools to implement in their practice
4. Set realistic expectations for AI performance in different legal scenarios

By providing this level of detail and clarity, GunnerBench aims to bridge the gap between AI capabilities and practical implementation in legal settings.

### Model Evaluation Process

The evaluation process is designed to be consistent and fair across all models. Key steps include:

1. Running each model against the same set of assignments using direct API access
2. Collecting structured outputs based on carefully designed prompts
3. Comparing model outputs against the gold standard answers
4. Calculating precision and recall scores
5. Aggregating scores across assignments to determine overall performance in each category and assign appropriate tiers

This process ensures that all models are evaluated under the same conditions, allowing for meaningful comparisons of their capabilities in legal tasks. The results are then presented in a clear, accessible format that enables legal professionals to quickly assess which models might best suit their needs.
